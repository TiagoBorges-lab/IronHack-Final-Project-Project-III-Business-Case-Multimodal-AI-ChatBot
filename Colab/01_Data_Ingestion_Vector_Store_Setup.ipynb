{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ebe9465",
      "metadata": {
        "id": "1ebe9465"
      },
      "source": [
        "### üíª **<i>Notebook 01:</i> Data Ingestion and Vector Store Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22df371d",
      "metadata": {
        "id": "22df371d"
      },
      "source": [
        "**For running on Google Colab only, run the code cell below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kh-Kf3Togx_w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh-Kf3Togx_w",
        "outputId": "47fe0c59-130f-4aaf-ee81-310b2239eb70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A carregar chaves API do Colab Secrets...\n",
            "‚úÖ Todas as chaves API foram carregadas com sucesso a partir do Colab Secrets.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "try:\n",
        "    # This import ONLY works on the Colab web interface\n",
        "    from google.colab import userdata\n",
        "\n",
        "    print(\"Loading API keys from Colab Secrets...\")\n",
        "\n",
        "    # --- Key Retrieval and Injection into os.environ ---\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ['GOOGLE_CSE_ID'] = userdata.get('GOOGLE_CSE_ID')\n",
        "    os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "    os.environ['LANGCHAIN_PROJECT'] = \"retrogaming-qa-bot\"\n",
        "\n",
        "    # --- Verification ---\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        raise ValueError(\"OPENAI_API_KEY was not found in Colab Secrets.\")\n",
        "\n",
        "    print(\"‚úÖ All API keys were successfully loaded from Colab Secrets.\")\n",
        "\n",
        "except ImportError:\n",
        "    # Fallback for local environments (local VS Code)\n",
        "    print(\"Loading API keys from local .env file (VS Code/Local Mode).\")\n",
        "    load_dotenv()\n",
        "\n",
        "# --- Common Configurations ---\n",
        "CHROMA_PATH = \"./chroma_db_retrogaming\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Here, the rest of the notebook variables are defined (llm, embeddings, etc.)\n",
        "# ... (should continue defining LLM/RAG configuration variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc086ce8",
      "metadata": {
        "id": "cc086ce8"
      },
      "source": [
        "##### **00 - Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cfa87d88",
      "metadata": {
        "id": "cfa87d88"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import YoutubeLoader, WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6814c4e",
      "metadata": {
        "id": "a6814c4e"
      },
      "source": [
        "##### **01 - Configuration, API keys Loadout, Yotube videos for processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "129054b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "129054b5",
        "outputId": "045d0957-c4f3-428b-a302-bdbf9b4aeecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration loaded. API Key is ready for use.\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "# 1. Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# 2. Get the API Key from the environment\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in your .env file.\")\n",
        "\n",
        "# 3. List of YouTube URLs (Retrogaming optimization)\n",
        "# ‚ö†Ô∏è ACTION REQUIRED: Replace these placeholders with your actual URLs.\n",
        "YOUTUBE_URLS = [\n",
        "    \"https://www.youtube.com/watch?v=zW1vpDQ9Ijs\",\n",
        "    \"https://www.youtube.com/watch?v=nXsHc0IUzLY\",\n",
        "    \"https://www.youtube.com/watch?v=i1KrbTkU1sw\",\n",
        "    \"https://www.youtube.com/watch?v=ktVc3So9XyY\",\n",
        "    \"https://www.youtube.com/watch?v=_IOdts-CszU\",\n",
        "    \"https://www.youtube.com/watch?v=xfegbR8UWqU\",\n",
        "    \"https://www.youtube.com/watch?v=XDDIBEdINAE\",\n",
        "    \"https://www.youtube.com/watch?v=cXEfpRsVUCk\",\n",
        "    \"https://www.youtube.com/watch?v=eztgBP-K_1k\",\n",
        "    \"https://www.youtube.com/watch?v=Ky-BuWEnS2A\",\n",
        "    \"https://www.youtube.com/watch?v=6KHEgkCN9yE\",\n",
        "    \"https://www.youtube.com/watch?v=eduuJml97Fc\",\n",
        "    \"https://www.youtube.com/watch?v=dLkkF4iXBBM\",\n",
        "    \"https://www.youtube.com/watch?v=eKV-phyaLFI\",\n",
        "    \"https://www.youtube.com/watch?v=ZV-yb2WMrQk\",\n",
        "    \"https://www.youtube.com/watch?v=ArstSQv0BVM\",\n",
        "    \"https://www.youtube.com/watch?v=oUcAsShb0gk\",\n",
        "    \"https://www.youtube.com/watch?v=rDeqmBIWTdk\",\n",
        "    \"https://www.youtube.com/watch?v=49SMKnMHNtU\",\n",
        "    \"https://www.youtube.com/watch?v=fXbtkg9-150\",\n",
        "    \"https://www.youtube.com/watch?v=GxjbvS8Jd_0\",\n",
        "    \"https://www.youtube.com/watch?v=cjzRlfOAEnU\",\n",
        "    \"https://www.youtube.com/watch?v=qefseBgp3Ns\",\n",
        "    \"https://www.youtube.com/watch?v=uvMDjf_Mmv4\",\n",
        "    \"https://www.youtube.com/watch?v=Fix6u4pksrg\",\n",
        "    \"https://www.youtube.com/watch?v=n-9DHfiS48A\",\n",
        "    \"https://www.youtube.com/watch?v=jvujTeMJ8Lg\",\n",
        "    \"https://www.youtube.com/watch?v=nkR5lc4tD5A\",\n",
        "    \"https://www.youtube.com/watch?v=Sue4VhQgEH0\",\n",
        "    \"https://www.youtube.com/watch?v=bE0SXJ7Vba8\",\n",
        "    \"https://www.youtube.com/watch?v=sR1_-HveQQY\",\n",
        "    \"https://www.youtube.com/watch?v=28u6RoYiCWI\",\n",
        "    \"https://www.youtube.com/watch?v=VD3LPvnadZY\",\n",
        "    \"https://www.youtube.com/watch?v=YY03OM7qfZ4\",\n",
        "    \"https://www.youtube.com/watch?v=7j7F-e-sels\",\n",
        "    \"https://www.youtube.com/watch?v=SiuNUlBz6yQ\",\n",
        "    \"https://www.youtube.com/watch?v=6IBDG_GJKFw\",\n",
        "    \"https://www.youtube.com/watch?v=UGeLqcwAjws\"\n",
        "]\n",
        "WEB_URLS = [\n",
        "    \"https://pcsx2.net/docs/usage/general/\",\n",
        "    \"https://pcsx2.net/docs\",\n",
        "\t\"https://pcsx2.net/docs/category/setup\",\n",
        "\t\"https://pcsx2.net/docs/setup/requirements\",\n",
        "\t\"https://pcsx2.net/docs/setup/bios\",\n",
        "\t\"https://pcsx2.net/docs/setup/discs\",\n",
        "\t\"https://pcsx2.net/docs/setup/running\",\n",
        "\t\"https://pcsx2.net/docs/category/configuration\",\n",
        "\t\"https://pcsx2.net/docs/configuration/general\",\n",
        "\t\"https://pcsx2.net/docs/configuration/controllers\",\n",
        "\t\"https://pcsx2.net/docs/configuration/memcards\",\n",
        "\t\"https://pcsx2.net/docs/category/troubleshooting\",\n",
        "\t\"https://pcsx2.net/docs/troubleshooting/general\",\n",
        "\t\"https://pcsx2.net/docs/troubleshooting/performance\",\n",
        "\t\"https://pcsx2.net/docs/troubleshooting/identify\",\n",
        "\t\"https://pcsx2.net/docs/troubleshooting/windows\",\n",
        "\t\"https://pcsx2.net/docs/troubleshooting/linux\",\n",
        "\t\"https://pt.dolphin-emu.org/?cr=pt\",\n",
        "\t\"https://dolphin-emu.org/download/\",\n",
        "\t\"https://dolphin-emu.org/docs/guides/\",\n",
        "\t\"https://dolphin-emu.org/compat/\",\n",
        "\t\"https://www.retroarch.com/\",\n",
        "\t\"https://www.retroarch.com/?page=platforms\",\n",
        "\t\"https://docs.libretro.com/start/understanding/\",\n",
        "\t\"https://docs.libretro.com/start/installation/\",\n",
        "\t\"https://www.mesen.ca/\",\n",
        "\t\"https://www.snes9x.com/\",\n",
        "\t\"https://www.snes9x.com/downloads.php\",\n",
        "\t\"https://mgba.io/\",\n",
        "\t\"https://mgba.io/downloads.html\",\n",
        "\t\"https://mgba.io/faq.html\"\n",
        "    ]\n",
        "# --- RAG Parameters ---\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 150\n",
        "CHROMA_PATH = \"./chroma_db_retrogaming\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "print(\"‚úÖ Configuration loaded. API Key is ready for use.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f49bb0",
      "metadata": {
        "id": "31f49bb0"
      },
      "source": [
        "##### **02 - Setup and Custom Transcript Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a5a9c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04a5a9c9",
        "outputId": "0f01a68a-a61b-4911-bbe7-6eba48c7a07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup and Custom Transcript functions defined. Ready for data loading loop.\n"
          ]
        }
      ],
      "source": [
        "import re # For extracting the video ID\n",
        "from langchain_core.documents import Document\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
        "\n",
        "# Initialize the API client once\n",
        "ytt_api = YouTubeTranscriptApi()\n",
        "\n",
        "# Function to extract Video ID from the URL\n",
        "def extract_video_id(url):\n",
        "    \"\"\"Extracts the video ID from a cleaned YouTube URL.\"\"\"\n",
        "    # Pattern to find 'v=' followed by the ID, stopping at '&' or end of string\n",
        "    match = re.search(r'(?<=v=)[a-zA-Z0-9_-]+', url)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "# Function to fetch transcript with language fallback\n",
        "def get_transcript_text(video_id, languages=['en', 'es', 'pt']):\n",
        "    \"\"\"Fetches and formats transcript using the updated API syntax.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the instance method 'fetch' with language preferences (descending priority)\n",
        "        transcript = ytt_api.fetch(video_id, languages=languages)\n",
        "    except Exception as e:\n",
        "        # Re-raise the exception to be handled in the main loop\n",
        "        raise e\n",
        "\n",
        "    full_text = ''\n",
        "    for snippet in transcript.snippets:\n",
        "        full_text += snippet.text + ' '\n",
        "\n",
        "    return full_text.strip()\n",
        "\n",
        "print(\"‚úÖ Setup and Custom Transcript functions defined. Ready for data loading loop.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2feebb",
      "metadata": {
        "id": "7c2feebb"
      },
      "source": [
        "##### **03 - Custom LangChain Data Loading Loop**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ea0c0b",
      "metadata": {},
      "source": [
        "**Option 1: For a firs-time running, run the below code cell to generate the CSV file<br>If a csv file already exists use Option 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad4aaa3",
      "metadata": {
        "id": "1ad4aaa3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#new 1 runner\n",
        "# --- NEW UNIFIED LOADING (Replaces Cell 10/First_Runner) ---\n",
        "# The goal is to create a single list of documents (YouTube and Web) for the CSV.\n",
        "\n",
        "def load_all_data_and_create_csv():\n",
        "    \"\"\"Loads data from YouTube and the Web, creates Documents, and saves to a CSV.\"\"\"\n",
        "    all_docs_raw = []\n",
        "    \n",
        "    # 1. Load YouTube Transcripts\n",
        "    print(f\"\\n[1/3] Starting transcript loading for {len(YOUTUBE_URLS)} videos...\")\n",
        "    for url in YOUTUBE_URLS:\n",
        "        video_id = extract_video_id(url)\n",
        "        if not video_id:\n",
        "            print(f\"‚ùå Failed to extract ID from URL: {url}\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            transcript_text = get_transcript_text(video_id)\n",
        "            all_docs_raw.append({\n",
        "                'source': url,\n",
        "                'source_type': 'youtube_transcript',\n",
        "                'video_id': video_id,\n",
        "                'content': transcript_text\n",
        "            })\n",
        "            print(f\"‚úÖ Loaded transcript for ID {video_id}.\")\n",
        "        except (NoTranscriptFound, TranscriptsDisabled):\n",
        "            print(f\"‚ùå Failed (No Transcript): ID {video_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed (Generic Error): ID {video_id} - {e}\")\n",
        "\n",
        "    # 2. Load Web Pages\n",
        "    print(f\"\\n[2/3] Starting web page loading for {len(WEB_URLS)} pages...\")\n",
        "    web_loader = WebBaseLoader(WEB_URLS)\n",
        "    \n",
        "    try:\n",
        "        web_docs = web_loader.load()\n",
        "        for doc in web_docs:\n",
        "            all_docs_raw.append({\n",
        "                'source': doc.metadata.get('source'),\n",
        "                'source_type': 'web_page',\n",
        "                'video_id': 'N/A',\n",
        "                'content': doc.page_content\n",
        "            })\n",
        "        print(f\"‚úÖ Loaded {len(web_docs)} web page documents.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load web pages: {e}\")\n",
        "        \n",
        "    # 3. Save to Unified CSV\n",
        "    df = pd.DataFrame(all_docs_raw)\n",
        "    TRANSCRIPT_CSV_PATH = \"./retrogaming_knowledge_cached.csv\"\n",
        "    df.to_csv(TRANSCRIPT_CSV_PATH, index=False)\n",
        "    \n",
        "    print(f\"\\n[3/3] ‚úÖ Complete. Unified data from {len(df)} sources saved to {TRANSCRIPT_CSV_PATH}.\")\n",
        "    \n",
        "    # Returns the DataFrame for the next step (although 'Second_Runner' is the default)\n",
        "    return df\n",
        "\n",
        "# Uncomment the line below to run the loading and create the CSV (First_Runner)\n",
        "df_all = load_all_data_and_create_csv()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dde9b5f6",
      "metadata": {},
      "source": [
        "**Option 2: Loads the local CSV file previously created**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93db66f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93db66f8",
        "outputId": "cbbcd5f9-310a-4b8d-f96b-6049cc375be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ A carregar todos os documentos do CSV em cache: ./retrogaming_knowledge_cached.csv\n",
            "\n",
            "Total de documentos LangChain carregados: 64\n"
          ]
        }
      ],
      "source": [
        "#--- Second_Runner (Reads the CSV and creates LangChain Documents) ---\n",
        "\n",
        "TRANSCRIPT_CSV_PATH = \"./retrogaming_knowledge_cached.csv\"\n",
        "all_docs = [] # Final list of LangChain Documents\n",
        "\n",
        "if os.path.exists(TRANSCRIPT_CSV_PATH):\n",
        "    print(f\"\\n‚úÖ Loading all documents from the cached CSV: {TRANSCRIPT_CSV_PATH}\")\n",
        "    df_cached = pd.read_csv(TRANSCRIPT_CSV_PATH)\n",
        "\n",
        "    for index, row in df_cached.iterrows():\n",
        "        content = str(row['content']) if pd.notna(row['content']) else \"\"\n",
        "\n",
        "        if content:\n",
        "            doc = Document(\n",
        "                page_content=content,\n",
        "                metadata={\n",
        "                    'source': row['source'],\n",
        "                    'source_type': row['source_type'],\n",
        "                    'video_id': row['video_id']\n",
        "                }\n",
        "            )\n",
        "            all_docs.append(doc)\n",
        "\n",
        "    print(f\"\\nTotal LangChain documents loaded: {len(all_docs)}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå CRITICAL ERROR: The cached data file was not found. Run 'load_all_data_and_create_csv()' and try again.\")\n",
        "    # Do not raise an error to allow the user to manually run the loading block.#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "566100d9",
      "metadata": {
        "id": "566100d9"
      },
      "source": [
        "##### **04 - Split Documents (Chunking)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2edbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2edbcd",
        "outputId": "c194f906-a71a-46b3-f536-23b1464068bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting document chunking...\n",
            "‚úÖ Split 64 documents into 582 total chunks.\n"
          ]
        }
      ],
      "source": [
        "# --- Imports needed for this section ---\n",
        "# Assuming these were imported in the first block:\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 2. Split the documents into smaller chunks\n",
        "print(\"Starting document chunking...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,       # Defined as 1000 in Section 1\n",
        "    chunk_overlap=CHUNK_OVERLAP, # Defined as 150 in Section 1\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(all_docs)\n",
        "print(f\"‚úÖ Split {len(all_docs)} documents into {len(chunks)} total chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f154160",
      "metadata": {
        "id": "3f154160"
      },
      "source": [
        "##### **05 - Create Embeddings and Store (Vector Store)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07933d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07933d83",
        "outputId": "09ff5803-5877-471c-af87-483901a4a6cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting embedding generation and storage in ./chroma_db_retrogaming...\n",
            "‚úÖ Successfully created and persisted vector store with 582 embeddings.\n",
            "   Vector store saved to: ./chroma_db_retrogaming\n",
            "\n",
            "Notebook 01 is complete. You can now proceed to Notebook 02!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3554025669.py:19: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vector_db.persist()\n"
          ]
        }
      ],
      "source": [
        "# --- Imports needed for this section ---\n",
        "# Assuming these were imported in the first block:\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# 3. Create the Embeddings Model and Vector Store\n",
        "print(f\"Starting embedding generation and storage in {CHROMA_PATH}...\")\n",
        "# The OpenAIEmbeddings class automatically uses the OPENAI_API_KEY from the environment\n",
        "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
        "\n",
        "# Create a persistent ChromaDB instance\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=CHROMA_PATH\n",
        ")\n",
        "\n",
        "# Persist the database to disk so it can be reloaded in Notebook 02\n",
        "vector_db.persist()\n",
        "\n",
        "print(f\"‚úÖ Successfully created and persisted vector store with {vector_db._collection.count()} embeddings.\")\n",
        "print(f\"   Vector store saved to: {CHROMA_PATH}\")\n",
        "print(\"\\nNotebook 01 is complete. You can now proceed to Notebook 02!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
